{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4cbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取所有文件名并进行分词\n",
    "folder_path = 'E:/Desktop/papers/all_papers/'\n",
    "word_list = []\n",
    "for fname in os.listdir(folder_path):\n",
    "    cutwords = word_tokenize(fname)\n",
    "#     print(cutwords)\n",
    "    for i in cutwords:\n",
    "        t = re.sub('.txt','',i) #去除.txt后缀\n",
    "        word_list.append(t.lower())\n",
    "    with open(folder_path + fname,encoding='utf-8',errors='ignore') as f:\n",
    "        abtract = f.read()\n",
    "    cut_abtract_words = word_tokenize(abtract)\n",
    "    for i in cut_abtract_words:\n",
    "        word_list.append(i.lower())       \n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e254d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除标点符号\n",
    "interpunctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']   #定义标点符号列表\n",
    "cutwords1 = [word for word in word_list if word not in interpunctuations]\n",
    "len(cutwords1),cutwords1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3114145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#去停用词\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "cutwords2 = [word for word in cutwords1 if word not in stops]\n",
    "len(cutwords2),cutwords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62762849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#词干提取\n",
    "from nltk.stem import PorterStemmer\n",
    "cutwords3 = []\n",
    "for cutword in cutwords2:\n",
    "    cutwords3.append(PorterStemmer().stem(cutword))\n",
    "len(cutwords3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#词性还原\n",
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cutwords4 = []\n",
    "for cutword in cutwords3:\n",
    "    cutwords4.append(WordNetLemmatizer().lemmatize(cutword, pos='n'))\n",
    "len(cutwords4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读进文件\n",
    "with open('E:/Desktop/papers/title_word_dict.txt', 'wb') as fw:\n",
    "    for i in range(len(cutwords4)):\n",
    "        fw.write(cutwords1[i].encode('utf-8'))\n",
    "        fw.write('\\n'.encode('utf-8'))\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练词向量\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim.models as word2vec\n",
    "\n",
    "sentences = LineSentence('E:/Desktop/papers/title_word_dict.txt')\n",
    "model = word2vec.Word2Vec(sentences, vector_size=100, sg=1, window=5,min_count=3,workers=4)\n",
    "model.save('E:/Desktop/papers/title_word2vec.model')\n",
    "model.wv.save_word2vec_format('E:/Desktop/papers/title_corpusDone.vector', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc507d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型\n",
    "w2v_path = 'E:/Desktop/papers/title_word2vec.model'\n",
    "def load_word2vec_model(w2_path):\n",
    "    model = word2vec.Word2Vec.load(w2_path)\n",
    "    return model\n",
    "model = load_word2vec_model(w2v_path)\n",
    "print(model.wv['pre-train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b108a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename):   # 绘制词向量图\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    print('绘制词向量中......')\n",
    "    plt.figure(figsize=(10, 10))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\t# 画点，对应low_dim_embs中每个词向量\n",
    "        plt.annotate(label,\t# 显示每个点对应哪个单词\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3782cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_txt_vec(file, threshold=0, dtype='float'):\t# 读取txt文件格式的word2vec词向量\n",
    "    print('读取词向量文件中......')\n",
    "    header = file.readline().split(' ')\n",
    "    count = int(header[0]) if threshold <= 0 else min(threshold, int(header[0]))\n",
    "    dim = int(header[1])\n",
    "    words = []\n",
    "    matrix = np.empty((count, dim), dtype=dtype)\n",
    "    for i in range(count):\n",
    "        word, vec = file.readline().split(' ', 1)\n",
    "        words.append(word)\n",
    "        matrix[i] = np.fromstring(vec, sep=' ', dtype=dtype)\n",
    "    return (words, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    w2v_txt_file = open('E:/Desktop/papers/title_corpusDone.vector','r',encoding='utf-8')\n",
    "    words, vectors = load_txt_vec(w2v_txt_file)\n",
    "    tsne = TSNE(perplexity=50,n_iter=5000,method='exact',n_components=2)\n",
    "    plot_only = 200\n",
    "    low_dim_embs = tsne.fit_transform(vectors[:plot_only])\n",
    "    labels = [words[i] for i in range(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, 'w2v.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计词频\n",
    "d = {}\n",
    "for i in cutwords4:\n",
    "    if i in d:\n",
    "        d[i] += 1\n",
    "    else:\n",
    "        d[i] = 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297acc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将字典转换成列表，并排序\n",
    "items = list(d.items())\n",
    "items.sort(key = lambda x:x[1], reverse=True)\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "chiyun = []\n",
    "for i in range(100):\n",
    "    word, count = items[i]\n",
    "    chiyun.append(word)\n",
    "len(chiyun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#词云展示\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text_cut = '/'.join(chiyun)\n",
    "wcloud = WordCloud(background_color='white', width=1000,max_words=1000, height=800, margin=2).generate(text_cut)\n",
    "plt.imshow(wcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
